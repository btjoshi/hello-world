{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression With Maximum Likelihood Estimation\n",
    "\n",
    "Linear regression is a classical model for predicting a numerical quantity. \n",
    "- The parameters of a linear regression model can be estimated using a least squares procedure or by a MLE procedure. \n",
    "- Supervised learning can be framed as a conditional probability problem, and MLE can be used to fit the parameters of a model that best summarizes the conditional probability distribution, so-called conditional maximum likelihood estimation. \n",
    "\n",
    "Linear Regression is a model that maps one or more numerical inputs to a numerical output. In terms of predictive modeling, it is suited to regression type problems: that is, the prediction of a real-valued quantity. The input data is denoted as X with n examples and the output is denoted y with one output for each input. \n",
    "\n",
    "The prediction of the model for a given input is denoted as yhat.      $yhat = model(X)$\n",
    "The model is defined in terms of parameters called coeficients (beta or$\\beta$), where there is one coeficient per input and an additional coeficient that provides the intercept or bias.\n",
    "\n",
    "The model can also be described using linear algebra, with a vector for the coeficients ($\\beta$) and a matrix for the input data (X) and a vector for the output (y).\n",
    "\n",
    "$ y = X * \\beta $\n",
    "\n",
    "The sample is known to be incomplete being drawn from broader population. Also measurement error or statistical noise is expected. The regression problem is to estimate the parameters of the model ($\\beta$) from the sample (nosiy. Two frameworks that are most common:\n",
    "1. Least Squares Optimization - by seeking a set of parameters that results in the smallest squared error between the predictions of the model (yhat) and the actual outputs (y), averaged over all examples in the dataset, or mean squared error.\n",
    "2. Maximum Likelihood Estimation - is frequentist probabilistic framework that seeks a set of parameters for the model that maximize a likelihood function. \n",
    "\n",
    "Under both frameworks, different optimization algorithms may be used, such as local search methods like the BFGS algorithm (or variants), and general optimization methods like stochastic gradient descent. The linear regression model is special in that an analytical solution also exists, meaning that the coeffcients can be calculated directly using linear algebra. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelyhood Estimation\n",
    "The joint probability distribution can be restated as the multiplication of the conditional probability for observing each example given the distribution parameters, as they are iid. Multiplying many small probabilities together can be unstable; hence it is common to restate this problem as the sum of the natural log conditional probability.\n",
    "\n",
    "Given the common use of log in the likelihood function, it is referred to as a log-likelihood function. It is also common in optimization problems to prefer to minimize the $cost function$ rather than to maximize it. Therefore, the negative of the log-likelihood function is used, referred to generally as a Negative Log-Likelihood (NLL) function.\n",
    "\n",
    "#  MLE Relationship to Machine Learning\n",
    "We can frame the problem of fitting a machine learning model as the problem of probability density estimation.\n",
    "1. The choice of model and model parameters is referred to as a modeling hypothesis h.\n",
    "2. Problem involves finding h that best explains the data X, or P(X; h)\n",
    "or more fully\n",
    "\n",
    "max $\\sum_{i=1}^{n}$ $log P(x_i ; h)$\n",
    "\n",
    "Now we can replace h with our linear regression model. Assuming (i.i.d.) and that the target variable (y) has statistical noise\n",
    "with a Gaussian distribution, zero mean, and the same variance for all examples, we can frame the problem of estimating y given X as estimating the mean value for $y$ from a Gaussian probability distribution given X.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Logistic Regression With Maximum Likelihood Estimation\n",
    "\n",
    "Logistic regression is a classical linear method for $binary$ classification. Logistic regression has a lot in common with linear regression, both techniques model the target variable with a line (or hyperplane, depending on the number of dimensions of input. Linear regression fits the line to the data, which can be used to predict a new quantity, whereas logistic regression fits a line to best separate the two classes. \n",
    "\n",
    "Model is identical to linear regression initially, then squashes the output of this weighted sum using a nonlinear function $ sigmoid$ to ensure the outputs are a value between 0 and 1. \n",
    "\n",
    "#  Logistic Regression and Log-Odds\n",
    "\n",
    "The linear part of the model (the weighted sum of the inputs) calculates the log-odds of a successful event. Specifically, the log-odds that a sample belongs to class 1, for the input variables at each level (all observed values). \n",
    "\n",
    "Odds are often stated as wins to losses (wins : losses), e.g. a one to ten chance or ratio of winning is stated as 1:10. Given the probability of success (p) predicted by the logistic regression model, we can convert it to odds of success as the probability of success divided by the probability of not success: p / (1-p) \n",
    "\n",
    "The logarithm of the odds is the log-odds and may be referred to as the logit (logistic unit), a unit of measure.\n",
    "\n",
    "So problem of fitting a ML model is a problem of probability density estimation. Choice of model and model parameters is referred to as a modeling hypothesis h, and the problem involves finding h that best explains the data X.\n",
    "\n",
    "The probability distribution that is most often used when there are two classes is the binomial distribution. This distribution has a single parameter, p, that is the probability of an event or a specific class. The basis for the likelihood function\n",
    "for a specific input, where the probability is given by the model (yhat) and the actual label is given from the dataset.\n",
    "\n",
    "likelihood = yhat * y + (1 - yhat) * (1 - y) \n",
    "\n",
    "This function will always return a large probability when the model is close to the matching class value, and a small value when it is far away, for both y = 0 and y = 1 cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 1.0, yhat=0.9, likelihood=0.9\n",
      "y= 1.0, yhat=0.1, likelihood=0.1\n",
      "y= 0.0, yhat=0.1, likelihood=0.9\n",
      "y= 0.0, yhat=0.9, likelihood=0.1\n"
     ]
    }
   ],
   "source": [
    "# test for Bernoulli likelihood function\n",
    "#Def likelihood (y, yhat): #spellerror!!\n",
    "def likelihood(y, yhat):\n",
    "    return yhat * y + (1 - yhat) * (1-y)\n",
    "\n",
    "# test for y = 1\n",
    "y, yhat = 1, 0.9\n",
    "print('y= %.1f, yhat=%.1f, likelihood=%.1f' % (y, yhat, likelihood(y, yhat)))\n",
    "y, yhat = 1, 0.1\n",
    "print('y= %.1f, yhat=%.1f, likelihood=%.1f' % (y, yhat, likelihood(y, yhat)))\n",
    "y, yhat = 0, 0.1\n",
    "print('y= %.1f, yhat=%.1f, likelihood=%.1f' % (y, yhat, likelihood(y, yhat)))\n",
    "y, yhat = 0, 0.9\n",
    "print('y= %.1f, yhat=%.1f, likelihood=%.1f' % (y, yhat, likelihood(y, yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common practice to minimize a cost function for optimization problems; we can invert the function so that we minimize the negative log-likelihood.\n",
    "Calculating the negative of the log-likelihood function for the Bernoulli distribution is equivalent to calculating the cross-entropy function for the Bernoulli distribution, where p() represents the probability of class 0 or class 1, and q() represents the estimation of the probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization (EM) Algo\n",
    "\n",
    "MLE involves treating the problem as an optimization or search problem, where we seek a set of parameters that results in the best fit for the joint probability of the data sample. \n",
    "A limitation is that it assumes that the dataset is complete, or fully observed. There may be datasets where only some of the relevant variables can be observed, and some cannot, and although they infuence other random variables in the dataset, they remain hidden. More generally, these unobserved or hidden variables are referred to as latent variables.\n",
    "\n",
    "The Expectation-Maximization algorithm provides an alternative approach. \n",
    "\n",
    "The EM algorithm is an iterative approach that cycles between two modes. The first model attempts to estimate the missing or latent variables, called the estimation-step or E-step. The second mode attempts to optimize the parameters of the model to best explain the data, called the maximization-step or M-step.\n",
    "\n",
    "# Gaussian Mixture Model and the EM Algorithm\n",
    "\n",
    "A mixture model is contains an unspecified combination of multiple probability distribution functions. The Gaussian Mixture Model (GMM), uses a combination of Gaussian (Normal) probability distributions and requires the estimation of the mean and\n",
    "standard deviation parameters for each.\n",
    "\n",
    "There are many techniques for estimating the parameters for a GMM, although a maximum likelihood estimate is perhaps the most common. \n",
    "\n",
    "In the EM algorithm, the estimation-step would estimate a value for the process latent variable for each data point, and the maximization step would optimize the parameters of the probability distributions in an attempt to best capture the density of the data. The process is repeated until a good set of latent values and a maximum likelihood is achieved that fits the\n",
    "data. \n",
    "\n",
    "If the number of processes was not known, a range of different numbers of components could be tested and the model with the best fit could be chosen, where models could be evaluated using scores such as Akaike or Bayesian Information Criterion (AIC or BIC) \n",
    "\n",
    "There are also many ways we can configure the model to incorporate other information we may know about the data, such as how to estimate initial values for the distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1\n",
      " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0\n",
      " 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# constuing a bimodal distribution\n",
    "from numpy import hstack\n",
    "from numpy.random import normal\n",
    "from matplotlib import pyplot\n",
    "#from sklearn.mixture import GuassianMixture ## spellerror!!\n",
    "from sklearn.mixture import GaussianMixture\n",
    "X1, X2 = normal(loc=20, scale=5, size=300), normal(loc=40, scale=5, size=700)\n",
    "X = hstack((X1, X2))\n",
    "X = X.reshape((len(X), 1))\n",
    "# fit model\n",
    "#model = GuassianMixture(n_components=2, init_params='random') ##Spellerror!!\n",
    "model = GaussianMixture(n_components=2, init_params='random')\n",
    "model.fit(X)\n",
    "#predict  learned values\n",
    "yhat = model.predict(X)\n",
    "print(yhat[:100])\n",
    "print(yhat[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Probabilistic Model Selection with AIC, BIC, and MDL\n",
    "\n",
    "Model selection: estimating the performance of different models in order to choose the best one. Common methods\n",
    "1. Fit candidate models on a training set, tuning them on the validation dataset, and selecting a model that performs the best according to metric, such as accuracy or error. But this requires a lot of data.\n",
    "2. Resampling techniques attempt to achieve the same as the train/val/test approach using a small dataset. In k-fold cross-validation a training set is split into many train/test pairs and a model is fit and evaluated on each. This is repeated for each model and a model is selected with the best average score across the k-folds. \n",
    "\n",
    "But in both above only model performance is assessed, regardless of model complexity.\n",
    "\n",
    "An alternative approach is to use probabilistic model selection (or information critiria) to quantify both performance on the training dataset and complexity. \n",
    "\n",
    "These statistics do not require a hold-out test set, but they do not take the uncertainty of the models into account and may end-up selecting models that are too simple. Model performance is evaluated by using a probabilistic framework, such as log-likelihood under MLE and complexity may be evaluated as the number of degrees of freedom or parameters in the model.\n",
    "\n",
    "There are three statistical approaches to estimating model fit and complexity, each can be shown to be equivalent or proportional to each other, although each was derived from a different framing or field of study. \n",
    "\n",
    "1. $Akaike Information Criterion (AIC)$ named after Hirotugu Akaike. Derived from frequentist probability but cannot be interpreted as an approximation to the marginal likelihood.       $ AIC = [- 2/N * LL] + 2 * [k/N] $\n",
    "\n",
    "N = number of examples in the training dataset, LL is the log-likelihood dataset, and k = number of parameters. \n",
    "\n",
    "Model with the lowest AIC is selected. Compared to the BIC method, the AIC statistic penalizes complex models less,\n",
    "meaning that it may put more emphasis on model performance on the training dataset, hence may select elect more complex models.\n",
    "\n",
    "2. $Bayesian Information Criterion (BIC)$ is derived from Bayesian probability. Like AIC, it is appropriate for models fit under the MLE. The BIC statistic is calculated for logistic regression as  $ BIC = [-2 * LL] + [log(N) * k] $\n",
    "\n",
    "Model with the lowest BIC is selected. BIC can be shown to be proportional to AIC. Unlike the AIC, the BIC penalizes the model more for its complexity. Bayesian probability framework means that given a family of models, including the true model, the probability correct model is chosen approaches 1 as the sample size increases. A downside is that for smaller, less representative training datasets, it is more likely to choose models that are too simple.\n",
    "\n",
    "3. $Minimum Description Length (MDL)$ Derived from information theory, is the minimum number of bits required to represent the data and the model.\n",
    "\n",
    "The MDL statistic is $ MDL = L(h) + L(D | h) $ where L() is the bits; h is the model, D is the predictions by the model, model with the lowest MDL is selected. \n",
    "\n",
    "The number of bits required can be calculated as the negative log-likelihood; $ MDL = -log(P(\\theta)) - log(P(y|X; \\theta)) $\n",
    "\n",
    "The MDL calculation is very similar to BIC and can be shown to be equivalent in some situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3\n",
      "MSE: 0.007\n",
      "AIC: -492.585\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from math import log\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#calculating AIC for regression\n",
    "def calculate_aic(n, mse, num_parama):\n",
    "    aic = n * log(mse) + 2 * num_params\n",
    "    return aic\n",
    "\n",
    "#generate a test dataset\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=0.1)\n",
    "\n",
    "#define and fit model on all data\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "#number of parameters\n",
    "num_params = len(model.coef_)+1\n",
    "print('Number of parameters: %d' % (num_params))\n",
    "\n",
    "#predict the training set and calculate the error\n",
    "yhat = model.predict(X)\n",
    "mse = mean_squared_error(y, yhat)\n",
    "print('MSE: %.3f' % (mse))\n",
    "\n",
    "aic = calculate_aic(len(y), mse, num_params)\n",
    "print('AIC: %.3f' %(aic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
